{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "desafio-2-mq.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jZamXsFJiD7s",
        "GxuRSt7LiD8B",
        "ZI_5pDhQiD8Q",
        "ofun5rdliD8d",
        "KJLusZNmiD8g",
        "j6AvWnGFiD8l",
        "nboHYxPpiD8n"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quiteria89/Deploy-Web/blob/master/desafio_2_mq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "4nOjUCzLiD66",
        "colab_type": "text"
      },
      "source": [
        "# MARATONA BEHIND THE CODE 2020\n",
        "\n",
        "## DESAFIO 2: PARTE 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsyhC7vCiD67",
        "colab_type": "text"
      },
      "source": [
        "### Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_GhHc60iD68",
        "colab_type": "text"
      },
      "source": [
        "Em projetos de ciência de dados visando a construção de modelos de *machine learning*, ou aprendizado estatístico, é muito incomum que os dados iniciais estejam já no formato ideal para a construção de modelos. São necessários vários passos intermediários de pré-processamento de dados, como por exemplo a codificação de variáveis categóricas, normalização de variáveis numéricas, tratamento de dados faltantes, etc. A biblioteca **scikit-learn** -- uma das mais populares bibliotecas de código-aberto para *machine learning* no mundo -- possui diversas funções já integradas para a realização das transformações de dados mais utilizadas. Entretanto, em um fluxo comum de um modelo de aprendizado de máquina, é necessária a aplicação dessas transformações pelo menos duas vezes: a primeira vez para \"treinar\" o modelo, e depois novamente quando novos dados forem enviados como entrada para serem classificados por este modelo. \n",
        "\n",
        "Para facilitar o trabalho com esse tipo de fluxo, o scikit-learn possui também uma ferramenta chamada **Pipeline**, que nada mais é do que uma lista ordenada de transformações que devem ser aplicadas nos dados. Para auxiliar no desenvolvimento e no gerenciamento de todo o ciclo-de-vida dessas aplicações, alem do uso de Pipelines, as equipes de cientistas de dados podem utilizar em conjunto o **Watson Machine Learning**, que possui dezenas de ferramentas para treinar, gerenciar, hospedar e avaliar modelos baseados em aprendizado de máquina. Além disso, o Watson Machine Learning é capaz de encapsular pipelines e modelos em uma API pronta para uso e integração com outras aplicações.\n",
        "\n",
        "Durante o desafio 2, você participante irá aprender a construir uma **Pipeline** para um modelo de classificação e hospedá-lo como uma API com o auxílio do Watson Machine Learning. Uma vez hospedado, você poderá integrar o modelo criado com outras aplicações, como assistentes virtuais e muito mais. Neste notebook, será apresentado um exemplo funcional de criação de um modelo e de uma pipeline no scikit-learn (que você poderá utilizar como template para a sua solução!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcdWs2yriD69",
        "colab_type": "text"
      },
      "source": [
        "### Trabalhando com Pipelines do scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-vfX6UpiD6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Primeiro, realizamos a instalação do scikit-learn versão 0.20.0 no Kernel deste notebook:\n",
        "!pip install scikit-learn==0.20.0 --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAJ9_amGiD7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Em seguida iremos importar diversas bibliotecas que serão utilizadas:\n",
        "\n",
        "# Pacote para trabalhar com JSON\n",
        "import json\n",
        "\n",
        "# Pacote para realizar requisições HTTP\n",
        "import requests\n",
        "\n",
        "# Pacote para exploração e análise de dados\n",
        "import pandas as pd\n",
        "\n",
        "# Pacote com métodos numéricos e representações matriciais\n",
        "import numpy as np\n",
        "\n",
        "# Pacote para construção de modelo baseado na técnica Gradient Boosting\n",
        "import xgboost as xgb\n",
        "\n",
        "# Pacotes do scikit-learn para pré-processamento de dados\n",
        "# \"SimpleImputer\" é uma transformação para preencher valores faltantes em conjuntos de dados\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Pacotes do scikit-learn para treinamento de modelos e construção de pipelines\n",
        "# Método para separação de conjunto de dados em amostras de treino e teste\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Método para criação de modelos baseados em árvores de decisão\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Classe para a criação de uma pipeline de machine-learning\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Pacotes do scikit-learn para avaliação de modelos\n",
        "# Métodos para validação cruzada do modelo criado\n",
        "from sklearn.model_selection import KFold, cross_validate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtcVEzDqiD7G",
        "colab_type": "text"
      },
      "source": [
        "### Importando um .csv de seu projeto no IBM Cloud Pak for Data para o Kernel deste notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMq1qIjOiD7H",
        "colab_type": "text"
      },
      "source": [
        "Primeiro iremos importar o dataset fornecido para o desafio, que já está incluso neste projeto!\n",
        "\n",
        "Você pode realizar a importação dos dados de um arquivo .csv diretamente para o Kernel do notebook como um DataFrame da biblioteca Pandas, muito utilizada para a manipulação de dados em Python.\n",
        "\n",
        "Para realizar a importação, basta selecionar a próxima célula e seguir as instruções na imagem abaixo:\n",
        "\n",
        "![alt text](https://i.imgur.com/K1DwL9I.png \"importing-csv-as-df\")\n",
        "\n",
        "Após a seleção da opção **\"Insert to code\"**, a célula abaixo será preenchida com o código necessário para importação e leitura dos dados no arquivo .csv como um DataFrame Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eVIEJKrjiD7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "<< INSIRA O DATASET COMO UM PANDAS DATAFRAME NESTA CÉLULA! >>>\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8sWDF2FiD7K",
        "colab_type": "text"
      },
      "source": [
        "Temos 15 colunas presentes no dataset fornecido, sendo dezessete delas variáveis características (dados de entrada) e um delas uma variável-alvo (que queremos que o nosso modelo seja capaz de prever). \n",
        "\n",
        "As variáveis características são:\n",
        "\n",
        "    MATRICULA       - número de matrícula do estudante\n",
        "    NOME            - nome completo do estudante\n",
        "    REPROVACOES_DE  - número de reprovações na disciplina de ``Direito Empresarial``\n",
        "    REPROVACOES_EM  - número de reprovações na disciplina de ``Empreendedorismo``\n",
        "    REPROVACOES_MF  - número de reprovações na disciplina de ``Matemática Financeira``\n",
        "    REPROVACOES_GO  - número de reprovações na disciplina de ``Gestão Operacional``\n",
        "    NOTA_DE         - média simples das notas do aluno na disciplina de ``Direito Empresarial`` (0-10)\n",
        "    NOTA_EM         - média simples das notas do aluno na disciplina de ``Empreendedorismo`` (0-10)\n",
        "    NOTA_MF         - média simples das notas do aluno na disciplina de ``Matemática Financeira`` (0-10)\n",
        "    NOTA_GO         - média simples das notas do aluno na disciplina de ``Gestão Operacional`` (0-10)\n",
        "    INGLES          - variável binária que indica se o estudante tem conhecimento em língua inglesa (0 -> sim ou 1 -> não).\n",
        "    H_AULA_PRES     - horas de estudo presencial realizadas pelo estudante\n",
        "    TAREFAS_ONLINE  - número de tarefas online entregues pelo estudante\n",
        "    FALTAS          - número de faltas acumuladas do estudante (todas disciplinas)\n",
        "    \n",
        "A variável-alvo é:\n",
        "\n",
        "    PERFIL               - uma *string* que indica uma de cinco possibilidades: \n",
        "        \"EXCELENTE\"      - Estudante não necessita de mentoria\n",
        "        \"MUITO BOM\"      - Estudante não necessita de mentoria\n",
        "        \"HUMANAS\"        - Estudante necessita de mentoria exclusivamente em matérias com conteúdo de ciências humanas\n",
        "        \"EXATAS\"         - Estudante necessita de mentoria apenas em disciplinas com conteúdo de ciências exatas\n",
        "        \"DIFICULDADE\"    - Estudante necessita de mentoria em duas ou mais disciplinas\n",
        "        \n",
        "Com um modelo capaz de classificar um estudante em uma dessas categorias, podemos automatizar parte da mentoria estudantil através de assistentes virtuais, que serão capazes de recomendar práticas de estudo e conteúdo personalizado com base nas necessidades de cada aluno."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vO6ez5XiD7L",
        "colab_type": "text"
      },
      "source": [
        "### Explorando os dados fornecidos\n",
        "\n",
        "Podemos continuar a exploração dos dados fornecidos com a função ``info()``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MqM0t4ZiD7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_1.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Ra4DfViD7P",
        "colab_type": "text"
      },
      "source": [
        "É notado que existem variáveis do tipo ``float64`` (números \"decimais\"), variáveis do tipo ``int64`` (números inteiros) e do tipo ``object`` (nesse caso são *strings*, ou texto). \n",
        "\n",
        "Como a maioria dos algoritmos de aprendizado estatístico supervisionado só aceita valores numéricos como entrada, é necessário então o pré-processamento das variáveis do tipo \"object\" antes de usar esse dataset como entrada para o treinamento de um modelo. Também é notado que existem valores faltantes em várias colunas. Esses valores faltantes também devem ser tratados antes de serem construídos modelos com esse conjunto de dados base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKde3VHuiD7Q",
        "colab_type": "text"
      },
      "source": [
        "A função ``describe()`` gera várias informações sobre as variáveis numéricas que também podem ser úteis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYfGO3qqiD7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_1.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0kUUE3BiD7V",
        "colab_type": "text"
      },
      "source": [
        "### Visualizações\n",
        "\n",
        "Para visualizar o dataset fornecido, podemos utilizar as bibliotecas ``matplotlib`` e ``seaborn``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygfLUvfXiD7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "274KilYOiD7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n",
        "\n",
        "sns.countplot(ax=axes[0], x='REPROVACOES_DE', data=df_data_1)\n",
        "sns.countplot(ax=axes[1], x='REPROVACOES_EM', data=df_data_1)\n",
        "sns.countplot(ax=axes[2], x='REPROVACOES_MF', data=df_data_1)\n",
        "sns.countplot(ax=axes[3], x='REPROVACOES_GO', data=df_data_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGqBFhXOiD7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n",
        "\n",
        "sns.distplot(df_data_1['NOTA_DE'], ax=axes[0])\n",
        "sns.distplot(df_data_1['NOTA_EM'], ax=axes[1])\n",
        "sns.distplot(df_data_1['NOTA_MF'], ax=axes[2])\n",
        "sns.distplot(df_data_1['NOTA_GO'].dropna(), ax=axes[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "mwMd423ciD7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(28, 4))\n",
        "\n",
        "sns.countplot(ax=axes[0], x='INGLES', data=df_data_1)\n",
        "sns.countplot(ax=axes[1], x='FALTAS', data=df_data_1)\n",
        "sns.countplot(ax=axes[2], x='H_AULA_PRES', data=df_data_1)\n",
        "sns.countplot(ax=axes[3], x='TAREFAS_ONLINE', data=df_data_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVXalzbJiD7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.plot()\n",
        "sns.countplot(x='PERFIL', data=df_data_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPsI39xqiD7q",
        "colab_type": "text"
      },
      "source": [
        "### Realizando o pré-processamento dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu59CVgsiD7r",
        "colab_type": "text"
      },
      "source": [
        "Para o pré-processamento dos dados serão apresentadas duas transformações básicas neste notebook, demonstrando a construção de uma Pipeline com um modelo funcional. Esta Pipeline funcional fornecida deverá ser melhorada pelo participante para que o modelo final alcance a maior acurácia possível, garantindo uma pontuação maior no desafio. Essa melhoria pode ser feita apenas no pré-processamento dos dados, na escolha de um algoritmo para treinamento de modelo diferente, ou até mesmo na alteração do *framework* usado (entretanto só será fornecido um exemplo pronto de integração do Watson Machine Learning com o *scikit-learn*).\n",
        "\n",
        "A primeira transformação (passo na nossa Pipeline) será a exclusão da coluna \"NOME\" do nosso dataset, que além de não ser uma variável numérica, também não é uma variável relacionada ao desempenho dos estudantes nas disciplinas. Existem funções prontas no scikit-learn para a realização dessa transformação, entretanto nosso exemplo irá demonstrar como criar uma transformação personalizada do zero no scikit-learn. Se desejado, o participante poderá utilizar esse exemplo para criar outras transformações e adicioná-las à Pipeline final :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZamXsFJiD7s",
        "colab_type": "text"
      },
      "source": [
        "#### Transformação 1: excluindo colunas do dataset\n",
        "\n",
        "Para a criação de uma transformação de dados personalizada no scikit-learn, é necessária basicamente a criação de uma classe com os métodos ``transform`` e ``fit``. No método transform será executada a lógica da nossa transformação.\n",
        "\n",
        "Na próxima célula é apresentado o código completo de uma transformação ``DropColumns`` para a remoção de colunas de um DataFrame pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzNzbkh9iD7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "# All sklearn Transforms must have the `transform` and `fit` methods\n",
        "class DropColumns(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.columns = columns\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        # Primeiro realizamos a cópia do dataframe 'X' de entrada\n",
        "        data = X.copy()\n",
        "        # Retornamos um novo dataframe sem as colunas indesejadas\n",
        "        return data.drop(labels=self.columns, axis='columns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-zgDC-siD7v",
        "colab_type": "text"
      },
      "source": [
        "Para aplicar essa transformação em um DataFrame pandas, basta instanciar um objeto *DropColumns* e chamar o método transform()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drDNlN2niD7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instanciando uma transformação DropColumns\n",
        "rm_columns = DropColumns(\n",
        "    columns=[\"NOME\"]  # Essa transformação recebe como parâmetro uma lista com os nomes das colunas indesejadas\n",
        ")\n",
        "\n",
        "print(rm_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJG4lzNciD7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizando as colunas do dataset original\n",
        "print(\"Colunas do dataset original: \\n\")\n",
        "print(df_data_1.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXpsK8bxiD72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Aplicando a transformação ``DropColumns`` ao conjunto de dados base\n",
        "rm_columns.fit(X=df_data_1)\n",
        "\n",
        "# Reconstruindo um DataFrame Pandas com o resultado da transformação\n",
        "df_data_2 = pd.DataFrame.from_records(\n",
        "    data=rm_columns.transform(\n",
        "        X=df_data_1\n",
        "    ),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBJtXDnviD76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizando as colunas do dataset transformado\n",
        "print(\"Colunas do dataset após a transformação ``DropColumns``: \\n\")\n",
        "print(df_data_2.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIuHe78qiD8A",
        "colab_type": "text"
      },
      "source": [
        "Nota-se que a coluna \"NOME\" foi removida e nosso dataset agora poossui apenas 17 colunas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxuRSt7LiD8B",
        "colab_type": "text"
      },
      "source": [
        "#### Transformação 2: tratando dados faltantes\n",
        "\n",
        "Para tratar os dados faltantes em nosso conjunto de dados, iremos agora utilizar uma transformação pronta da biblioteca scikit-learn, chamada **SimpleImputer**.\n",
        "\n",
        "Essa transformação permite diversas estratégias para o tratamento de dados faltantes. A documentação oficial pode ser encontrada em: https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "\n",
        "Neste exemplo iremos simplesmente transformar todos os valores faltantes em zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sepj-9FhiD8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criação de um objeto ``SimpleImputer``\n",
        "si = SimpleImputer(\n",
        "    missing_values=np.nan,  # os valores faltantes são do tipo ``np.nan`` (padrão Pandas)\n",
        "    strategy='constant',  # a estratégia escolhida é a alteração do valor faltante por uma constante\n",
        "    fill_value=0,  # a constante que será usada para preenchimento dos valores faltantes é um int64=0.\n",
        "    verbose=0,\n",
        "    copy=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Agd7LuDiD8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizando os dados faltantes do dataset após a primeira transformação (df_data_2)\n",
        "print(\"Valores nulos antes da transformação SimpleImputer: \\n\\n{}\\n\".format(df_data_2.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFD-lcrCiD8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Aplicamos o SimpleImputer ``si`` ao conjunto de dados df_data_2 (resultado da primeira transformação)\n",
        "si.fit(X=df_data_2)\n",
        "\n",
        "# Reconstrução de um novo DataFrame Pandas com o conjunto imputado (df_data_3)\n",
        "df_data_3 = pd.DataFrame.from_records(\n",
        "    data=si.transform(\n",
        "        X=df_data_2\n",
        "    ),  # o resultado SimpleImputer.transform(<<pandas dataframe>>) é lista de listas\n",
        "    columns=df_data_2.columns  # as colunas originais devem ser conservadas nessa transformação\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfucyeiMiD8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizando os dados faltantes do dataset após a segunda transformação (SimpleImputer) (df_data_3)\n",
        "print(\"Valores nulos no dataset após a transformação SimpleImputer: \\n\\n{}\\n\".format(df_data_3.isnull().sum(axis = 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsrqqH23iD8M",
        "colab_type": "text"
      },
      "source": [
        "Nota-se que não temos mais nenhum valor faltante no nosso conjunto de dados :)\n",
        "\n",
        "Vale salientar que nem sempre a alteração dos valores faltantes por 0 é a melhor estratégia. O participante é incentivado a estudar e implementar estratégias diferentes de tratamento dos valores faltantes para aprimorar seu modelo e melhorar sua pontuação final."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ6PgiyTiD8N",
        "colab_type": "text"
      },
      "source": [
        "### Treinando um modelo de classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIDmoUaiD8N",
        "colab_type": "text"
      },
      "source": [
        "Finalizado o pré-processamento, já temos o conjunto de dados no formato necessário para o treinamento do nosso modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AVcC7KGiD8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_data_3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDiwroD4iD8Q",
        "colab_type": "text"
      },
      "source": [
        "No exemplo fornecido, iremos utilizar todas as colunas, exceto a coluna **LABELS** como *features* (variáveis de entrada).\n",
        "\n",
        "A variável **LABELS** será a variável-alvo do modelo, conforme descrito no enunciado do desafio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI_5pDhQiD8Q",
        "colab_type": "text"
      },
      "source": [
        "#### Definindo as features do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSWlxvzbiD8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definição das colunas que serão features (nota-se que a coluna NOME não está presente)\n",
        "features = [\n",
        "    \"MATRICULA\", 'REPROVACOES_DE', 'REPROVACOES_EM', \"REPROVACOES_MF\", \"REPROVACOES_GO\",\n",
        "    \"NOTA_DE\", \"NOTA_EM\", \"NOTA_MF\", \"NOTA_GO\",\n",
        "    \"INGLES\", \"H_AULA_PRES\", \"TAREFAS_ONLINE\", \"FALTAS\", \n",
        "]\n",
        "\n",
        "# Definição da variável-alvo\n",
        "target = [\"PERFIL\"]\n",
        "\n",
        "# Preparação dos argumentos para os métodos da biblioteca ``scikit-learn``\n",
        "X = df_data_3[features]\n",
        "y = df_data_3[target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGAYo25BiD8W",
        "colab_type": "text"
      },
      "source": [
        "O conjunto de entrada (X):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWbXvKxCiD8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-2bqGWjiD8a",
        "colab_type": "text"
      },
      "source": [
        "As variáveis-alvo correspondentes (y):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8TS8D1HiD8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofun5rdliD8d",
        "colab_type": "text"
      },
      "source": [
        "#### Separando o dataset em um conjunto de treino e um conjunto de teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk0zPPrMiD8d",
        "colab_type": "text"
      },
      "source": [
        "Iremos separar o dataset fornecido em dois grupos: um para treinar nosso modelo, e outro para testarmos o resultado através de um teste cego. A separação do dataset pode ser feita facilmente com o método *train_test_split()* do scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx5ojemyiD8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separação dos dados em um conjunto de treino e um conjunto de teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=337)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJLusZNmiD8g",
        "colab_type": "text"
      },
      "source": [
        "#### Criando um modelo baseado em árvores de decisão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol2MyDsdiD8h",
        "colab_type": "text"
      },
      "source": [
        "No exemplo fornecido iremos criar um classificador baseado em **árvores de decisão**.\n",
        "\n",
        "O primeiro passo é basicamente instanciar um objeto *DecisionTreeClassifier()* da biblioteca scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "248JzpC2iD8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criação da árvore de decisão com a biblioteca ``scikit-learn``:\n",
        "dtc_model = DecisionTreeClassifier()  # O modelo será criado com os parâmetros padrões da biblioteca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7Rfde_iD8k",
        "colab_type": "text"
      },
      "source": [
        "Material teórico sobre árvores de decisão na documentação oficial do scikit-learn: https://scikit-learn.org/stable/modules/tree.html\n",
        "\n",
        "Um guia para iniciantes no mundo do machine learning: https://developer.ibm.com/br/articles/cc-beginner-guide-machine-learning-ai-cognitive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6AvWnGFiD8l",
        "colab_type": "text"
      },
      "source": [
        "#### Execução do evento de treino de uma árvore de decisão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZrPLgeziD8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Treino do modelo (é chamado o método *fit()* com os conjuntos de treino)\n",
        "dtc_model.fit(\n",
        "    X_train,\n",
        "    y_train\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nboHYxPpiD8n",
        "colab_type": "text"
      },
      "source": [
        "#### Execução de predições e avaliação do modelo criado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MafrdQlHiD8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Realização de teste cego no modelo criado\n",
        "y_pred = dtc_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gvEannDiD8u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5UbuzZ5iD8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_qgE7X8iD8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Acurácia alcançada pela árvore de decisão\n",
        "print(\"Acurácia: {}%\".format(100*round(accuracy_score(y_test, y_pred), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y3kQ-FtiD81",
        "colab_type": "text"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDA18us8iD82",
        "colab_type": "text"
      },
      "source": [
        "Neste notebook foi demonstrado como trabalhar com transformações e modelos com a biblioteca scikit-learn. É recomendado que o participante realize seus experimentos editando o código fornecido aqui até que um modelo com acurácia elevada seja alcançado.\n",
        "\n",
        "Quando você estiver satisfeito com seu modelo, pode passar para a segunda etapa do desafio -- encapsular seu modelo como uma API REST pronta para uso com o Watson Machine Learning!\n",
        "\n",
        "O notebook para a segunda etapa já se encontra neste projeto, basta acessar a aba **ASSETS** e inicializá-lo! Não se esqueca de antes desligar o Kernel deste notebook para reduzir o consumo de sua camada grátis do IBM Cloud Pak for Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSw4yoXriD82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}